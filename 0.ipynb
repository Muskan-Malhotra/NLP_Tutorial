{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image #to insert image in jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/home/shivangi/Pictures/NLP-image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Natural Language Processing? \n",
    "A field of computer science, artificial intelligence and computational linguistics concerned with \n",
    "the interactions between computers and human (natural) languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Also known as data wrangling, Data Cleaning\n",
    "\n",
    "Process of converting data from the initial raw form into another format, in order \n",
    "to prepare data for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to convert .txt file into list where each new line is an element of the list\n",
    "\n",
    "def read(path):\n",
    "    sample=open(path, 'r')\n",
    "    text= [line for line in sample.readlines()]\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# path='/home/shivangi/Downloads/sample_text'\n",
    "# sample= open(path, 'r')\n",
    "# text= [line for line in sample.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=read('sample_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n\",\n",
       " 'my name is shivangi.\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is shivangi.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A process by which big quantity of text is divided into smaller parts called tokens. \n",
    "\n",
    "A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing\n",
    "\n",
    "It is considered as an early step in the NLP process\n",
    "\n",
    "Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. \n",
    "\n",
    "Two types:\n",
    "* Sentence Tokenization\n",
    "* Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the method word_tokenize() to split a sentence into words  via unique space character\n",
    "delimeter used: white spaces\n",
    "\n",
    "Drawback:\n",
    "may also tokenize multi-word expressions like New York to ‘New’ and ‘York’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book.It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words=[word_tokenize(item) for item in data]\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.', \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\", 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=[sent_tokenize(item) for item in data]\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lorem Ipsum is simply dummy text of the printing and typesetting industry.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0] #sentences[0] is a list in itelf, So to get its first elemen, we sentences[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are those words which are filtered out before further processing of text, since these words contribute\n",
    "little to overall meaning, given that they are generally the most common words in a language. \n",
    "(such as “the”, “a”, “an”, “in”)\n",
    "\n",
    "Such words do not help in providing useful insight in a document but ends up taking a lot of space in the database, \n",
    "or consumes up a valuable amount of processing time\n",
    "\n",
    "NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. \n",
    "You can find them in the nltk_data directory: https://github.com/mitmedialab/DataBasic/tree/master/nltk_data/corpora/stopwords is the directory address.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shivangi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no. of stopwards in the given vovabulary: 179\n"
     ]
    }
   ],
   "source": [
    "print('total no. of stopwards in the given vovabulary:',len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence=[]\n",
    "for word in words:\n",
    "    filtered_sentence.append([w for w in word if not w in stop_words])\n",
    "filtered_sentence = [w for word in words for w in word if not w in stop_words]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RegexpTokenizer splits a string into substrings using a regular expression. \n",
    "\n",
    "\\w+ matches 1 or more word characters (same as [a-zA-Z0-9_]+)\n",
    "\n",
    "NOTE:---> \\w, \\W: ANY ONE word/non-word character. For ASCII, word characters are [a-zA-Z0-9_]\n",
    "\n",
    "reference: https://www.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my', 'name', 'is', 'shivangi']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct=[tokenizer.tokenize(item) for item in data]\n",
    "punct[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuation(s):\n",
    "#     list_punctuation = list(string.punctuation)\n",
    "#     for i in list_punctuation:\n",
    "#         s = s.replace(i,'')\n",
    "#     return s\n",
    "\n",
    "# tokens = [remove_punctuation(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing plays an important role in maintaining consistency of expected output\n",
    "\n",
    "### Advantages:\n",
    "* helps in solving sparsity issues- This issue arises where the same words with different cases map to the same lowercase form. Eg: shivangi, SHIVANGI, shiVaNGi----------> shivangi\n",
    "\n",
    "* lowercasing is very useful is for search- Suppose you are looking for word UK in a document but \n",
    "\n",
    "\n",
    "Another example where lowercasing is very useful is for search. Imagine, you are looking for documents containing “usa”. However, no results were showing up because “usa” was indexed as “USA”. Now, who should we blame? The U.I. designer who set-up the interface or the engineer who set-up the search index?\n",
    "\n",
    "While lowercasing should be standard practice, I’ve also had situations where preserving the capitalization was important. For example, in predicting the programming language of a source code file. The word System in Java is quite different from system in python. Lowercasing the two makes them identical, causing the classifier to lose important predictive features. While lowercasing is generally helpful, it may not be applicable for all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower=[item.lower() for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum.\\n\",\n",
       " 'my name is shivangi.\\n']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-numeric words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphanumeric characters are those comprised of the combined set of the 26 alphabetic characters, A to Z, \n",
    "and the 10 Arabic numerals, 0 to 9.\n",
    "\n",
    "Non alpha-numeric characters:\n",
    "!\t$+$\t$<$\t[\n",
    "%\t,\t$<=$\t]\n",
    "&\t-\t$<>$\t|\n",
    "'\t.\t=\t~\n",
    "(\t/\t==\t~=\n",
    ")\t/!\t$>$\t(space)\n",
    "*\t//\t$>$=\t(return)\n",
    "*!\t{\t?\t`\n",
    "**\t}\t@\t:\n",
    ";\t^\t|=\t&=\n",
    "+=\t-=\t*=\t/=\n",
    "**=\t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_words=[]\n",
    "for item in data:\n",
    "    alpha_words.append([word for word in item.split() if not word.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['industry.',\n",
       "  \"industry's\",\n",
       "  '1500s,',\n",
       "  'book.It',\n",
       "  'centuries,',\n",
       "  'typesetting,',\n",
       "  'unchanged.',\n",
       "  '1960s',\n",
       "  'passages,',\n",
       "  'Ipsum.'],\n",
       " ['shivangi.']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%appos.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text normalization technique in NLP  i.e stemming and normalization\n",
    "\n",
    "Inflection: In grammar, inflection is the modification of a word to express different grammatical categories \n",
    "such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more \n",
    "grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\n",
    "\n",
    "eg. playing, plays, played----> play\n",
    "Sentence example: the boy's car has different colors-----> the boy car has differ color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Applications:\n",
    "Stemming and Lemmatization are widely used in tagging systems, indexing, SEOs, Web search results,\n",
    "and information retrieval. For example, searching for fish on Google will also result in\n",
    "fishes, fishing as fish is the stem of both words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define: Stemming is the process of reducing inflection in words to their root forms such as mapping\n",
    "a group of words to the same stem even if the stem itself is not a valid word in the Language.\n",
    "    \n",
    "Stem (root) is the part of the word to which you add inflectional (changing/deriving) \n",
    "affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words \n",
    "that are not actual words. Stems are created by removing the suffixes or prefixes used with a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has stemmer for both English and Non-English\n",
    "Computer program that stems a word is called as stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
